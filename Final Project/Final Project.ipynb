{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecccc91b",
   "metadata": {},
   "source": [
    "# Final Project \n",
    "\n",
    "By : Priyank Sai Pannem (XR97612)\n",
    "     Bhanu Sri Somani ( GN38499)\n",
    " \n",
    "Professor: Mehmet Sarica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02864bca",
   "metadata": {},
   "source": [
    "Datasets : https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/X_train.csv\n",
    "           https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/y_train.csv\n",
    "\n",
    "The main objective of this project is to create a sentiment forecasting model for movie reviews. We have two datasets containing favorable and negative comments, and our goal is to develop a machine learning model that can accurately predict the sentiment expressed in these reviews. By achieving this objective, we aim to gain insights into audience reactions and opinions towards movies, which can be valuable for decision-making in the film industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2790e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/X_train.csv')\n",
    "y = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5d3966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shame, is a Swedish film in Swedish with Engli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I know it's rather unfair to comment on a movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Bread\" very sharply skewers the conventions o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After reading tons of good reviews about this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During the Civil war a wounded union soldier h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Shame, is a Swedish film in Swedish with Engli...          1\n",
       "1  I know it's rather unfair to comment on a movi...          0\n",
       "2  \"Bread\" very sharply skewers the conventions o...          1\n",
       "3  After reading tons of good reviews about this ...          1\n",
       "4  During the Civil war a wounded union soldier h...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X,y],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37069407",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d769d",
   "metadata": {},
   "source": [
    "### Function to remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af871b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        shame is a swedish film in swedish with englis...\n",
       "1        i know its rather unfair to comment on a movie...\n",
       "2        bread very sharply skewers the conventions of ...\n",
       "3        after reading tons of good reviews about this ...\n",
       "4        during the civil war a wounded union soldier h...\n",
       "                               ...                        \n",
       "39995    as a pagan i must say this movie has little if...\n",
       "39996    a lot of the comments seem to treat this film ...\n",
       "39997    ive only seen most of the series since i leave...\n",
       "39998    the all i have is 5 dollars and my wedding rin...\n",
       "39999    when king kong stripped her of her top in the ...\n",
       "Name: review, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return re.sub(r'[^\\w\\s]','',cleaned_text)\n",
    "X['review'] = X['review'].apply(clean)\n",
    "X['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ca774",
   "metadata": {},
   "source": [
    "The code takes a piece of text, which is assumed to contain HTML tags, and performs several operations to clean it. It removes the HTML tags, converts the text to lowercase, and removes any non-alphanumeric characters (except spaces). The resulting cleaned text is then assigned back to the 'review' column of the 'X' DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9ee8a",
   "metadata": {},
   "source": [
    "### Function to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59271f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        shame swedish film swedish english subtitles f...\n",
       "1        know rather unfair comment movie without seein...\n",
       "2        bread sharply skewers conventions horror movie...\n",
       "3        reading tons good reviews movie decided take s...\n",
       "4        civil war wounded union soldier hides isolated...\n",
       "                               ...                        \n",
       "39995    pagan must say movie little magickal significa...\n",
       "39996    lot comments seem treat film baseball movie fe...\n",
       "39997    ive seen series since leave tv background nois...\n",
       "39998    5 dollars wedding ring scene riot also guffawe...\n",
       "39999    king kong stripped top 1976 remake breathless ...\n",
       "Name: review, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stopwords from a text\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "X['review'] = X['review'].apply(remove_stopwords)\n",
    "X['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc473fdb",
   "metadata": {},
   "source": [
    "Stopwords are eliminated from the'review' column of the 'X' DataFrame using the aforementioned code. Stopwords are frequent words in the English language that have little to no meaning, such \"the,\" \"is,\" and \"a.\" The code breaks the text into tokens, examines each token to see if it is a stopword, and then filters out the stopwords. The DataFrame's'review' column is then given a new assignment for the output text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5fd1",
   "metadata": {},
   "source": [
    "## Function to convert text into root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee4e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        shame swedish film swedish english subtitle fi...\n",
       "1        know rather unfair comment movie without seein...\n",
       "2        bread sharply skewer convention horror movie g...\n",
       "3        reading ton good review movie decided take spi...\n",
       "4        civil war wounded union soldier hide isolated ...\n",
       "                               ...                        \n",
       "39995    pagan must say movie little magickal significa...\n",
       "39996    lot comment seem treat film baseball movie fee...\n",
       "39997    ive seen series since leave tv background nois...\n",
       "39998    5 dollar wedding ring scene riot also guffawed...\n",
       "39999    king kong stripped top 1976 remake breathless ...\n",
       "Name: review, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize a text\n",
    "def lemmatize_text(text):\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply lemmatization to the 'text' column\n",
    "X['review'] = X['review'].apply(lemmatize_text)\n",
    "X['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38d80e",
   "metadata": {},
   "source": [
    "The above code takes the 'review' column in the 'X' DataFrame, breaks the text into individual words, and converts those words to their base or dictionary form using lemmatization. The lemmatized words are then combined back into a single string and stored in the 'review' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eeae0d",
   "metadata": {},
   "source": [
    "## Converting text into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2b0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_processed = vectorizer.fit_transform(X['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ba03d",
   "metadata": {},
   "source": [
    "The code uses the TfidfVectorizer to convert the text in the 'review' column of the DataFrame 'X' into numerical features. It calculates the TF-IDF scores for each word in each review, which measure the importance of a word in a particular review relative to its frequency across all reviews. The resulting 'X_processed' matrix contains these transformed features, ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fef7ce",
   "metadata": {},
   "source": [
    "# Elbow plot to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e589891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# distortions = list()\n",
    "\n",
    "# for i in range(20, 50):\n",
    "#     print('Iteration:',i)\n",
    "#     km = KMeans(n_clusters=i, random_state=0)\n",
    "#     km = km.fit(X_train_processed)\n",
    "#     distortions.append(km.inertia_)\n",
    "    \n",
    "# plt.plot(range(20, 50), distortions, marker='o')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e74f0a",
   "metadata": {},
   "source": [
    "On the preprocessed data, the aforementioned code applies the K-means clustering method. The distortion value is calculated for each iteration when fitting the KMeans model over a range of cluster numbers. The best number of clusters is then chosen based on the \"elbow point\" in the plot and the consequent distortion values.\n",
    "Running the above code We got optimal number of clusters as 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4816238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8848889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 180881)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b65b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Normalize the TF-IDF vectors\n",
    "normalized_vectors = normalize(X_train)\n",
    "\n",
    "# Choose the number of clusters (k)\n",
    "k = 40\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(normalized_vectors)\n",
    "\n",
    "# Get the cluster labels for each document\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centroids\n",
    "cluster_centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b19006",
   "metadata": {},
   "source": [
    "The normalized TF-IDF vectors are subjected to k-means clustering in the code above. Each document is given a cluster, and the centroid (average vector) of each cluster is calculated. The cluster centroids provide the typical TF-IDF vector for each cluster, while the cluster labels specify which cluster each document belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99559569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9462,   964, 18155,  3549, 29145, 25503, 17561,    48, 19412,\n",
       "       16706, 13074,  8109, 11458,  8013, 22304, 26971, 23621, 14264,\n",
       "        9748, 13181, 23248,  3012,  5202,  2458,  3549, 18155, 19412,\n",
       "       16706, 28751, 13500,  1457, 26770, 18155,  3549, 15086,  3883,\n",
       "       12881,  5065, 11313,  9620,  4931, 26710,  1729,   335, 26000,\n",
       "        3210, 16827, 12233,  8196,  8835,  6593, 29453,  3755, 19656,\n",
       "         122,  8853, 26000, 16551, 16367, 27998, 20439, 19570,  5769,\n",
       "        5998, 16393, 30240, 11582, 31652, 26022, 20578,  1457, 23098,\n",
       "       13895,  1946, 31687, 13595,  9125,  9309,  5581, 14045],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=2).fit(X_train)\n",
    "\n",
    "# Get the indices of the two nearest points to each centroid\n",
    "_, indices = nbrs.kneighbors(cluster_centroids)\n",
    "actual_idx = indices.flatten()\n",
    "actual_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223caa06",
   "metadata": {},
   "source": [
    "The NearestNeighbors technique is used in the code above to determine the two locations that are closest to the centroid of each cluster. You may locate the precise data points connected to each centroid by using these indices, which describe the locations of the closest points in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1d92f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57fc87",
   "metadata": {},
   "source": [
    "<b> Sanity check <b/>\n",
    "- For every cluster we took 2 nearest neighbours. Since there are 40 clusters in total, the total number of labeled records are 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9db811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_df = pd.DataFrame()\n",
    "# mapping_df['Cluster'] = cluster_labels[actual_idx]\n",
    "# mapping_df['Sentiment'] =  y_train.iloc[actual_idx]['sentiment'].values\n",
    "# mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb5831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled = X_train[actual_idx]\n",
    "y_labeled = y_train.iloc[actual_idx]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a7d936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_labeled.value_counts().plot(kind='bar')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f676d30",
   "metadata": {},
   "source": [
    "The aforementioned code generates a bar plot that displays the number of various emotion labels. The sentiment categories are shown on the x-axis, while the counts for each sentiment category are shown on the y-axis. The height of each vertical bar, which graphically represents each sentiment category, correlates to the number of that sentiment label. The figure makes it easier to see how the dataset's sentiment labels are distributed.\n",
    "As, the from the above visulaisation now we know that data is un-biased hence the perfect metric to evaluate the model performance will be Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28c8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13079be1",
   "metadata": {},
   "source": [
    "## Model 1: Logestic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0164e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logistic__C': 0.1, 'logistic__penalty': 'l2'}\n",
      "Best score: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.17      0.28      3948\n",
      "           1       0.54      0.97      0.70      4052\n",
      "\n",
      "    accuracy                           0.57      8000\n",
      "   macro avg       0.69      0.57      0.49      8000\n",
      "weighted avg       0.69      0.57      0.49      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('logistic', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid_lr = {\n",
    "    'logistic__C': [0.1, 1, 10, 20],\n",
    "    'logistic__penalty': ['l2',]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_lr.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_score_lr = grid_search_lr.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params_lr)\n",
    "print(\"Best score:\", best_score_lr)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_lr = grid_search_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e219c",
   "metadata": {},
   "source": [
    "On the labeled training data, logistic regression with grid search is carried out using the code above. Cross-validation is used to find the optimal pairing of \"C\" (inverse of regularization strength) and \"penalty\" (kind of regularization) values. In order to assess the effectiveness of the model, a classification report is produced using the test data and the best model discovered by grid search.The accuracy is not upto to the expectations so let's try building RandomForest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043f949",
   "metadata": {},
   "source": [
    "## Model 2: RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1b488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__max_depth': 10, 'rf__min_samples_split': 5, 'rf__n_estimators': 100}\n",
      "Best score: 0.7625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.11      0.19      3948\n",
      "           1       0.53      0.98      0.69      4052\n",
      "\n",
      "    accuracy                           0.55      8000\n",
      "   macro avg       0.68      0.54      0.44      8000\n",
      "weighted avg       0.68      0.55      0.44      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'rf__max_depth': [None, 5, 10],  # Maximum depth of the trees\n",
    "    'rf__min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_rf.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params_rf)\n",
    "print(\"Best score:\", best_score_rf)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_rf = grid_search_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f31504",
   "metadata": {},
   "source": [
    "The code performs random forest classification with grid search on the labeled training data. It searches for the best combination of 'n_estimators' (number of trees), 'max_depth' (maximum depth of trees), and 'min_samples_split' (minimum samples to split) values using cross-validation. The best model found during grid search is used to make predictions on the test data, and a classification report is generated to evaluate the model's performance. \n",
    "There is not much change in the performance of decision tree than logistic Regression.\n",
    "So let's try building SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ef09d",
   "metadata": {},
   "source": [
    "## Model 3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a133e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'svm__C': 1, 'svm__kernel': 'rbf'}\n",
      "Best score: 0.7375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.03      0.06      3948\n",
      "           1       0.51      1.00      0.68      4052\n",
      "\n",
      "    accuracy                           0.52      8000\n",
      "   macro avg       0.70      0.51      0.37      8000\n",
      "weighted avg       0.70      0.52      0.37      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid_svc = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_svc.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_svc = grid_search_svc.best_params_\n",
    "best_score_svc = grid_search_svc.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params_svc)\n",
    "print(\"Best score:\", best_score_svc)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_svc = grid_search_svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48faff8",
   "metadata": {},
   "source": [
    "The above code performs support vector machine classification with grid search on the labeled training data. It searches for the best combination of 'C' (regularization parameter) and 'kernel' (type of kernel function) values using cross-validation. The best model found during grid search is used to make predictions on the test data, and a classification report is generated to evaluate the model's performance. \n",
    "Comparing all the model's We have come to a conclusion that Logistic Regression performed better with higher accuracy with any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabd861",
   "metadata": {},
   "source": [
    "## Testing the Best performed model on Production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ae8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/X_final.csv')\n",
    "y_final = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/final/y_final.csv')\n",
    "\n",
    "X_final_processed = vectorizer.transform(X_final['review'])\n",
    "\n",
    "\n",
    "y_pred = grid_search_lr.predict(X_final_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ef736",
   "metadata": {},
   "source": [
    "The code applies the trained TF-IDF vectorizer and logistic regression model to make predictions on the final test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3678e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.01      0.03      5000\n",
      "           1       0.50      1.00      0.67      5000\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.73      0.51      0.35     10000\n",
      "weighted avg       0.73      0.51      0.35     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_final, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5011e9",
   "metadata": {},
   "source": [
    "The code generates a classification report that provides insights into the performance of the classification model. It includes metrics like precision, recall, F1-score, and support for each class in the dataset. The zero_division parameter is used to handle cases where there might be a zero division error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6b7b7",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcce81d",
   "metadata": {},
   "source": [
    "From all the models we found that Logistic Regression Model performed the best with Accuracy of 57%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
